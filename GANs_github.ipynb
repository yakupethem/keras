{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyOB9O9UfHwOjUT9M2Z8BmwO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ga_yFTWH60FD","outputId":"db70e351-1faf-4365-d24b-1bf354b61b94","colab":{"base_uri":"https://localhost:8080/"}},"source":["from __future__ import print_function, division\n","\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt\n","\n","import sys\n","\n","import numpy as np\n","\n","class GAN():\n","    def __init__(self):\n","        self.img_rows = 28\n","        self.img_cols = 28\n","        self.channels = 1\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = 100\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        validity = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, validity)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(256, input_dim=self.latent_dim))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(1024))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n","        model.add(Reshape(self.img_shape))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Flatten(input_shape=self.img_shape))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(256))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(1, activation='sigmoid'))\n","        model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=128, sample_interval=50):\n","\n","        # Load the dataset\n","        (X_train, _), (_, _) = mnist.load_data()\n","\n","        # Rescale -1 to 1\n","        X_train = X_train / 127.5 - 1.\n","        X_train = np.expand_dims(X_train, axis=3)\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random batch of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","            # Generate a batch of new images\n","            gen_imgs = self.generator.predict(noise)\n","\n","            # Train the discriminator\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","            # Train the generator (to have the discriminator label samples as valid)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","\n","            # Plot the progress\n","            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","            # If at save interval => save generated image samples\n","            if epoch % sample_interval == 0:\n","                self.sample_images(epoch)\n","\n","    def sample_images(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        #plt.savefig(\"images/%d.png\" % epoch)\n","        \n","        plt.close()\n","\n","\n","if __name__ == '__main__':\n","    gan = GAN()\n","    gan.train(epochs=30000, batch_size=32, sample_interval=200)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1581 [D loss: 0.600482, acc.: 73.44%] [G loss: 1.075425]\n","1582 [D loss: 0.577739, acc.: 71.88%] [G loss: 1.106569]\n","1583 [D loss: 0.540329, acc.: 78.12%] [G loss: 0.959876]\n","1584 [D loss: 0.581329, acc.: 82.81%] [G loss: 0.956990]\n","1585 [D loss: 0.547658, acc.: 79.69%] [G loss: 0.944578]\n","1586 [D loss: 0.550913, acc.: 78.12%] [G loss: 1.024856]\n","1587 [D loss: 0.615030, acc.: 65.62%] [G loss: 0.980492]\n","1588 [D loss: 0.554295, acc.: 81.25%] [G loss: 0.974059]\n","1589 [D loss: 0.601256, acc.: 67.19%] [G loss: 0.981490]\n","1590 [D loss: 0.520266, acc.: 81.25%] [G loss: 0.995532]\n","1591 [D loss: 0.586144, acc.: 70.31%] [G loss: 1.009738]\n","1592 [D loss: 0.511874, acc.: 76.56%] [G loss: 0.951228]\n","1593 [D loss: 0.544615, acc.: 78.12%] [G loss: 0.968606]\n","1594 [D loss: 0.537353, acc.: 71.88%] [G loss: 0.918566]\n","1595 [D loss: 0.559705, acc.: 68.75%] [G loss: 1.041164]\n","1596 [D loss: 0.575116, acc.: 71.88%] [G loss: 1.038528]\n","1597 [D loss: 0.555797, acc.: 71.88%] [G loss: 0.981025]\n","1598 [D loss: 0.574809, acc.: 67.19%] [G loss: 1.013477]\n","1599 [D loss: 0.567011, acc.: 70.31%] [G loss: 1.004227]\n","1600 [D loss: 0.505669, acc.: 84.38%] [G loss: 0.983912]\n","1601 [D loss: 0.568693, acc.: 78.12%] [G loss: 0.894239]\n","1602 [D loss: 0.601644, acc.: 71.88%] [G loss: 0.954871]\n","1603 [D loss: 0.544916, acc.: 78.12%] [G loss: 0.930575]\n","1604 [D loss: 0.594148, acc.: 73.44%] [G loss: 0.985705]\n","1605 [D loss: 0.630148, acc.: 60.94%] [G loss: 1.015862]\n","1606 [D loss: 0.520696, acc.: 76.56%] [G loss: 1.076641]\n","1607 [D loss: 0.618781, acc.: 60.94%] [G loss: 0.904820]\n","1608 [D loss: 0.609871, acc.: 68.75%] [G loss: 0.995942]\n","1609 [D loss: 0.584525, acc.: 67.19%] [G loss: 1.081834]\n","1610 [D loss: 0.548304, acc.: 78.12%] [G loss: 0.986654]\n","1611 [D loss: 0.518658, acc.: 78.12%] [G loss: 0.958781]\n","1612 [D loss: 0.572685, acc.: 71.88%] [G loss: 1.006375]\n","1613 [D loss: 0.524549, acc.: 78.12%] [G loss: 1.069282]\n","1614 [D loss: 0.588339, acc.: 65.62%] [G loss: 0.987880]\n","1615 [D loss: 0.577778, acc.: 73.44%] [G loss: 0.972491]\n","1616 [D loss: 0.569448, acc.: 65.62%] [G loss: 0.983745]\n","1617 [D loss: 0.532299, acc.: 76.56%] [G loss: 1.013449]\n","1618 [D loss: 0.474181, acc.: 81.25%] [G loss: 1.067569]\n","1619 [D loss: 0.623254, acc.: 56.25%] [G loss: 1.021264]\n","1620 [D loss: 0.557990, acc.: 73.44%] [G loss: 1.053689]\n","1621 [D loss: 0.547794, acc.: 78.12%] [G loss: 1.083754]\n","1622 [D loss: 0.515760, acc.: 81.25%] [G loss: 1.064930]\n","1623 [D loss: 0.580036, acc.: 65.62%] [G loss: 0.947957]\n","1624 [D loss: 0.525428, acc.: 70.31%] [G loss: 0.980625]\n","1625 [D loss: 0.537659, acc.: 78.12%] [G loss: 1.016381]\n","1626 [D loss: 0.511765, acc.: 81.25%] [G loss: 1.023623]\n","1627 [D loss: 0.523043, acc.: 79.69%] [G loss: 1.012246]\n","1628 [D loss: 0.613302, acc.: 67.19%] [G loss: 0.926917]\n","1629 [D loss: 0.520765, acc.: 75.00%] [G loss: 0.906747]\n","1630 [D loss: 0.601715, acc.: 64.06%] [G loss: 0.897973]\n","1631 [D loss: 0.593208, acc.: 68.75%] [G loss: 0.892302]\n","1632 [D loss: 0.598487, acc.: 70.31%] [G loss: 0.931317]\n","1633 [D loss: 0.571511, acc.: 75.00%] [G loss: 1.003689]\n","1634 [D loss: 0.478514, acc.: 89.06%] [G loss: 0.998924]\n","1635 [D loss: 0.545727, acc.: 75.00%] [G loss: 1.062453]\n","1636 [D loss: 0.547958, acc.: 79.69%] [G loss: 1.064012]\n","1637 [D loss: 0.551072, acc.: 76.56%] [G loss: 1.072584]\n","1638 [D loss: 0.492049, acc.: 73.44%] [G loss: 1.045913]\n","1639 [D loss: 0.573289, acc.: 70.31%] [G loss: 0.911232]\n","1640 [D loss: 0.542410, acc.: 76.56%] [G loss: 1.024568]\n","1641 [D loss: 0.564440, acc.: 78.12%] [G loss: 1.067668]\n","1642 [D loss: 0.573431, acc.: 71.88%] [G loss: 1.015823]\n","1643 [D loss: 0.504477, acc.: 76.56%] [G loss: 1.023011]\n","1644 [D loss: 0.556500, acc.: 73.44%] [G loss: 0.998418]\n","1645 [D loss: 0.558987, acc.: 73.44%] [G loss: 1.068865]\n","1646 [D loss: 0.589258, acc.: 65.62%] [G loss: 1.019705]\n","1647 [D loss: 0.642298, acc.: 62.50%] [G loss: 1.083501]\n","1648 [D loss: 0.604355, acc.: 70.31%] [G loss: 1.037687]\n","1649 [D loss: 0.527117, acc.: 70.31%] [G loss: 0.965521]\n","1650 [D loss: 0.524866, acc.: 78.12%] [G loss: 0.941564]\n","1651 [D loss: 0.545711, acc.: 75.00%] [G loss: 0.919836]\n","1652 [D loss: 0.643972, acc.: 60.94%] [G loss: 0.952389]\n","1653 [D loss: 0.505317, acc.: 81.25%] [G loss: 0.959496]\n","1654 [D loss: 0.581476, acc.: 71.88%] [G loss: 1.011821]\n","1655 [D loss: 0.551461, acc.: 82.81%] [G loss: 1.021275]\n","1656 [D loss: 0.577224, acc.: 73.44%] [G loss: 1.096023]\n","1657 [D loss: 0.556153, acc.: 73.44%] [G loss: 1.029960]\n","1658 [D loss: 0.562927, acc.: 73.44%] [G loss: 1.043562]\n","1659 [D loss: 0.585692, acc.: 64.06%] [G loss: 1.029030]\n","1660 [D loss: 0.612189, acc.: 59.38%] [G loss: 0.999607]\n","1661 [D loss: 0.547795, acc.: 73.44%] [G loss: 0.983592]\n","1662 [D loss: 0.603452, acc.: 65.62%] [G loss: 0.961926]\n","1663 [D loss: 0.583989, acc.: 71.88%] [G loss: 0.960858]\n","1664 [D loss: 0.583358, acc.: 70.31%] [G loss: 1.047562]\n","1665 [D loss: 0.567647, acc.: 68.75%] [G loss: 1.048343]\n","1666 [D loss: 0.542085, acc.: 73.44%] [G loss: 1.021288]\n","1667 [D loss: 0.567296, acc.: 75.00%] [G loss: 0.946592]\n","1668 [D loss: 0.571297, acc.: 75.00%] [G loss: 0.920210]\n","1669 [D loss: 0.545802, acc.: 68.75%] [G loss: 1.043214]\n","1670 [D loss: 0.530075, acc.: 76.56%] [G loss: 1.022231]\n","1671 [D loss: 0.558597, acc.: 81.25%] [G loss: 0.942875]\n","1672 [D loss: 0.611581, acc.: 68.75%] [G loss: 0.947522]\n","1673 [D loss: 0.577333, acc.: 70.31%] [G loss: 0.952087]\n","1674 [D loss: 0.531667, acc.: 75.00%] [G loss: 1.012087]\n","1675 [D loss: 0.582229, acc.: 73.44%] [G loss: 0.984439]\n","1676 [D loss: 0.517127, acc.: 84.38%] [G loss: 1.027613]\n","1677 [D loss: 0.557569, acc.: 78.12%] [G loss: 1.027728]\n","1678 [D loss: 0.556166, acc.: 73.44%] [G loss: 0.894543]\n","1679 [D loss: 0.629398, acc.: 62.50%] [G loss: 0.944513]\n","1680 [D loss: 0.592396, acc.: 64.06%] [G loss: 0.979777]\n","1681 [D loss: 0.584962, acc.: 71.88%] [G loss: 0.994979]\n","1682 [D loss: 0.644807, acc.: 56.25%] [G loss: 0.977305]\n","1683 [D loss: 0.522418, acc.: 76.56%] [G loss: 1.053759]\n","1684 [D loss: 0.612534, acc.: 71.88%] [G loss: 0.985546]\n","1685 [D loss: 0.628274, acc.: 71.88%] [G loss: 0.976942]\n","1686 [D loss: 0.539719, acc.: 73.44%] [G loss: 0.970243]\n","1687 [D loss: 0.614346, acc.: 65.62%] [G loss: 0.978896]\n","1688 [D loss: 0.589315, acc.: 67.19%] [G loss: 0.987382]\n","1689 [D loss: 0.552759, acc.: 76.56%] [G loss: 0.937723]\n","1690 [D loss: 0.604675, acc.: 64.06%] [G loss: 0.964753]\n","1691 [D loss: 0.553189, acc.: 81.25%] [G loss: 0.945231]\n","1692 [D loss: 0.546617, acc.: 78.12%] [G loss: 1.013726]\n","1693 [D loss: 0.574531, acc.: 68.75%] [G loss: 1.017987]\n","1694 [D loss: 0.560856, acc.: 76.56%] [G loss: 0.916066]\n","1695 [D loss: 0.606761, acc.: 68.75%] [G loss: 0.958061]\n","1696 [D loss: 0.572437, acc.: 73.44%] [G loss: 0.946917]\n","1697 [D loss: 0.533809, acc.: 81.25%] [G loss: 0.995983]\n","1698 [D loss: 0.568074, acc.: 73.44%] [G loss: 1.095008]\n","1699 [D loss: 0.548489, acc.: 73.44%] [G loss: 0.997096]\n","1700 [D loss: 0.579480, acc.: 79.69%] [G loss: 0.945679]\n","1701 [D loss: 0.488752, acc.: 79.69%] [G loss: 0.979066]\n","1702 [D loss: 0.589456, acc.: 68.75%] [G loss: 0.980125]\n","1703 [D loss: 0.586378, acc.: 75.00%] [G loss: 0.878077]\n","1704 [D loss: 0.505510, acc.: 76.56%] [G loss: 1.043972]\n","1705 [D loss: 0.607981, acc.: 64.06%] [G loss: 1.000264]\n","1706 [D loss: 0.591975, acc.: 59.38%] [G loss: 1.007277]\n","1707 [D loss: 0.583394, acc.: 68.75%] [G loss: 1.039546]\n","1708 [D loss: 0.546394, acc.: 82.81%] [G loss: 0.941499]\n","1709 [D loss: 0.617229, acc.: 62.50%] [G loss: 0.958285]\n","1710 [D loss: 0.589870, acc.: 68.75%] [G loss: 0.995143]\n","1711 [D loss: 0.577736, acc.: 68.75%] [G loss: 0.907403]\n","1712 [D loss: 0.561920, acc.: 73.44%] [G loss: 1.027791]\n","1713 [D loss: 0.534802, acc.: 78.12%] [G loss: 1.017626]\n","1714 [D loss: 0.567372, acc.: 79.69%] [G loss: 1.122777]\n","1715 [D loss: 0.567979, acc.: 68.75%] [G loss: 0.942200]\n","1716 [D loss: 0.549045, acc.: 73.44%] [G loss: 1.045453]\n","1717 [D loss: 0.540170, acc.: 75.00%] [G loss: 1.012880]\n","1718 [D loss: 0.610760, acc.: 67.19%] [G loss: 1.001220]\n","1719 [D loss: 0.548963, acc.: 76.56%] [G loss: 0.996177]\n","1720 [D loss: 0.660366, acc.: 59.38%] [G loss: 0.978423]\n","1721 [D loss: 0.647562, acc.: 57.81%] [G loss: 1.111187]\n","1722 [D loss: 0.573083, acc.: 73.44%] [G loss: 0.953262]\n","1723 [D loss: 0.540438, acc.: 75.00%] [G loss: 0.876959]\n","1724 [D loss: 0.590552, acc.: 67.19%] [G loss: 0.967877]\n","1725 [D loss: 0.544164, acc.: 68.75%] [G loss: 0.898114]\n","1726 [D loss: 0.605991, acc.: 64.06%] [G loss: 0.949081]\n","1727 [D loss: 0.556259, acc.: 73.44%] [G loss: 0.965869]\n","1728 [D loss: 0.551157, acc.: 70.31%] [G loss: 0.993952]\n","1729 [D loss: 0.566744, acc.: 78.12%] [G loss: 1.023030]\n","1730 [D loss: 0.612362, acc.: 68.75%] [G loss: 0.981537]\n","1731 [D loss: 0.587523, acc.: 70.31%] [G loss: 0.956843]\n","1732 [D loss: 0.544201, acc.: 73.44%] [G loss: 1.043490]\n","1733 [D loss: 0.589787, acc.: 67.19%] [G loss: 0.952827]\n","1734 [D loss: 0.567449, acc.: 70.31%] [G loss: 0.950716]\n","1735 [D loss: 0.505345, acc.: 79.69%] [G loss: 1.034374]\n","1736 [D loss: 0.609371, acc.: 70.31%] [G loss: 0.974469]\n","1737 [D loss: 0.618455, acc.: 65.62%] [G loss: 0.965375]\n","1738 [D loss: 0.620081, acc.: 64.06%] [G loss: 1.031042]\n","1739 [D loss: 0.656634, acc.: 53.12%] [G loss: 0.939105]\n","1740 [D loss: 0.601916, acc.: 67.19%] [G loss: 0.887453]\n","1741 [D loss: 0.610289, acc.: 62.50%] [G loss: 0.963603]\n","1742 [D loss: 0.559239, acc.: 75.00%] [G loss: 0.912943]\n","1743 [D loss: 0.628565, acc.: 64.06%] [G loss: 0.988029]\n","1744 [D loss: 0.587776, acc.: 71.88%] [G loss: 1.028274]\n","1745 [D loss: 0.606038, acc.: 67.19%] [G loss: 1.035104]\n","1746 [D loss: 0.591954, acc.: 73.44%] [G loss: 0.945322]\n","1747 [D loss: 0.586567, acc.: 68.75%] [G loss: 0.956781]\n","1748 [D loss: 0.561139, acc.: 78.12%] [G loss: 0.962939]\n","1749 [D loss: 0.615404, acc.: 62.50%] [G loss: 0.968170]\n","1750 [D loss: 0.581626, acc.: 73.44%] [G loss: 0.996938]\n","1751 [D loss: 0.526272, acc.: 81.25%] [G loss: 0.947791]\n","1752 [D loss: 0.546916, acc.: 71.88%] [G loss: 0.980876]\n","1753 [D loss: 0.565505, acc.: 67.19%] [G loss: 1.022079]\n","1754 [D loss: 0.533733, acc.: 76.56%] [G loss: 1.042216]\n","1755 [D loss: 0.598028, acc.: 70.31%] [G loss: 1.028022]\n","1756 [D loss: 0.607960, acc.: 64.06%] [G loss: 1.007452]\n","1757 [D loss: 0.526521, acc.: 75.00%] [G loss: 1.004670]\n","1758 [D loss: 0.584206, acc.: 70.31%] [G loss: 1.004405]\n","1759 [D loss: 0.551134, acc.: 67.19%] [G loss: 0.934946]\n","1760 [D loss: 0.600464, acc.: 71.88%] [G loss: 1.027042]\n","1761 [D loss: 0.503507, acc.: 75.00%] [G loss: 1.009641]\n","1762 [D loss: 0.623378, acc.: 67.19%] [G loss: 1.040594]\n","1763 [D loss: 0.583420, acc.: 73.44%] [G loss: 1.045353]\n","1764 [D loss: 0.673204, acc.: 56.25%] [G loss: 0.948451]\n","1765 [D loss: 0.574476, acc.: 67.19%] [G loss: 1.004150]\n","1766 [D loss: 0.659156, acc.: 65.62%] [G loss: 0.989958]\n","1767 [D loss: 0.559266, acc.: 67.19%] [G loss: 0.967039]\n","1768 [D loss: 0.528735, acc.: 79.69%] [G loss: 0.999041]\n","1769 [D loss: 0.604141, acc.: 70.31%] [G loss: 0.904635]\n","1770 [D loss: 0.577066, acc.: 67.19%] [G loss: 0.890435]\n","1771 [D loss: 0.596097, acc.: 70.31%] [G loss: 0.928223]\n","1772 [D loss: 0.551850, acc.: 76.56%] [G loss: 1.047752]\n","1773 [D loss: 0.637526, acc.: 62.50%] [G loss: 0.903407]\n","1774 [D loss: 0.601124, acc.: 67.19%] [G loss: 0.997741]\n","1775 [D loss: 0.591520, acc.: 62.50%] [G loss: 1.021452]\n","1776 [D loss: 0.608002, acc.: 65.62%] [G loss: 0.975132]\n","1777 [D loss: 0.641369, acc.: 62.50%] [G loss: 1.046799]\n","1778 [D loss: 0.486456, acc.: 85.94%] [G loss: 1.089410]\n","1779 [D loss: 0.569908, acc.: 64.06%] [G loss: 1.021159]\n","1780 [D loss: 0.574176, acc.: 75.00%] [G loss: 1.121519]\n","1781 [D loss: 0.581816, acc.: 64.06%] [G loss: 0.973081]\n","1782 [D loss: 0.567564, acc.: 73.44%] [G loss: 0.928155]\n","1783 [D loss: 0.630153, acc.: 54.69%] [G loss: 0.949755]\n","1784 [D loss: 0.554433, acc.: 76.56%] [G loss: 0.991286]\n","1785 [D loss: 0.572396, acc.: 67.19%] [G loss: 1.014208]\n","1786 [D loss: 0.596721, acc.: 70.31%] [G loss: 1.086508]\n","1787 [D loss: 0.535685, acc.: 78.12%] [G loss: 0.998480]\n","1788 [D loss: 0.595779, acc.: 64.06%] [G loss: 1.030288]\n","1789 [D loss: 0.534543, acc.: 75.00%] [G loss: 0.992982]\n","1790 [D loss: 0.560961, acc.: 75.00%] [G loss: 1.039821]\n","1791 [D loss: 0.537199, acc.: 81.25%] [G loss: 0.991099]\n","1792 [D loss: 0.532217, acc.: 76.56%] [G loss: 0.942327]\n","1793 [D loss: 0.537343, acc.: 73.44%] [G loss: 0.954463]\n","1794 [D loss: 0.566449, acc.: 71.88%] [G loss: 0.977927]\n","1795 [D loss: 0.589912, acc.: 65.62%] [G loss: 0.980339]\n","1796 [D loss: 0.515781, acc.: 76.56%] [G loss: 0.984317]\n","1797 [D loss: 0.559784, acc.: 68.75%] [G loss: 1.074430]\n","1798 [D loss: 0.523010, acc.: 81.25%] [G loss: 0.937789]\n","1799 [D loss: 0.580164, acc.: 65.62%] [G loss: 1.009502]\n","1800 [D loss: 0.638614, acc.: 71.88%] [G loss: 0.915509]\n","1801 [D loss: 0.606848, acc.: 67.19%] [G loss: 0.953141]\n","1802 [D loss: 0.552127, acc.: 71.88%] [G loss: 0.948265]\n","1803 [D loss: 0.578639, acc.: 68.75%] [G loss: 0.960397]\n","1804 [D loss: 0.614503, acc.: 64.06%] [G loss: 1.025759]\n","1805 [D loss: 0.608762, acc.: 70.31%] [G loss: 0.932317]\n","1806 [D loss: 0.630975, acc.: 65.62%] [G loss: 0.956103]\n","1807 [D loss: 0.526579, acc.: 73.44%] [G loss: 0.976344]\n","1808 [D loss: 0.572262, acc.: 67.19%] [G loss: 0.999082]\n","1809 [D loss: 0.542727, acc.: 76.56%] [G loss: 0.983070]\n","1810 [D loss: 0.503099, acc.: 81.25%] [G loss: 1.048668]\n","1811 [D loss: 0.564711, acc.: 73.44%] [G loss: 0.951127]\n","1812 [D loss: 0.523602, acc.: 76.56%] [G loss: 1.011872]\n","1813 [D loss: 0.530698, acc.: 75.00%] [G loss: 1.023533]\n","1814 [D loss: 0.593386, acc.: 65.62%] [G loss: 1.065257]\n","1815 [D loss: 0.561562, acc.: 68.75%] [G loss: 0.994565]\n","1816 [D loss: 0.642627, acc.: 62.50%] [G loss: 1.004714]\n","1817 [D loss: 0.501292, acc.: 76.56%] [G loss: 1.125297]\n","1818 [D loss: 0.555148, acc.: 75.00%] [G loss: 1.023844]\n","1819 [D loss: 0.596929, acc.: 67.19%] [G loss: 0.935497]\n","1820 [D loss: 0.567384, acc.: 76.56%] [G loss: 0.899388]\n","1821 [D loss: 0.592715, acc.: 70.31%] [G loss: 1.028442]\n","1822 [D loss: 0.582302, acc.: 67.19%] [G loss: 0.929970]\n","1823 [D loss: 0.542442, acc.: 75.00%] [G loss: 0.946741]\n","1824 [D loss: 0.542820, acc.: 78.12%] [G loss: 0.967427]\n","1825 [D loss: 0.517081, acc.: 75.00%] [G loss: 1.040907]\n","1826 [D loss: 0.521878, acc.: 75.00%] [G loss: 0.968688]\n","1827 [D loss: 0.518884, acc.: 78.12%] [G loss: 1.010952]\n","1828 [D loss: 0.524969, acc.: 81.25%] [G loss: 1.042691]\n","1829 [D loss: 0.597549, acc.: 68.75%] [G loss: 1.052374]\n","1830 [D loss: 0.586900, acc.: 67.19%] [G loss: 1.021203]\n","1831 [D loss: 0.650159, acc.: 62.50%] [G loss: 0.890374]\n","1832 [D loss: 0.640218, acc.: 56.25%] [G loss: 0.974564]\n","1833 [D loss: 0.552192, acc.: 71.88%] [G loss: 0.975319]\n","1834 [D loss: 0.504344, acc.: 79.69%] [G loss: 1.052224]\n","1835 [D loss: 0.615267, acc.: 65.62%] [G loss: 1.012897]\n","1836 [D loss: 0.553506, acc.: 70.31%] [G loss: 1.044287]\n","1837 [D loss: 0.560490, acc.: 67.19%] [G loss: 0.926222]\n","1838 [D loss: 0.602664, acc.: 71.88%] [G loss: 0.874710]\n","1839 [D loss: 0.575453, acc.: 75.00%] [G loss: 0.928654]\n","1840 [D loss: 0.559586, acc.: 71.88%] [G loss: 1.022999]\n","1841 [D loss: 0.522693, acc.: 81.25%] [G loss: 0.995685]\n","1842 [D loss: 0.551672, acc.: 73.44%] [G loss: 1.002795]\n","1843 [D loss: 0.580403, acc.: 65.62%] [G loss: 1.038197]\n","1844 [D loss: 0.564545, acc.: 71.88%] [G loss: 0.983529]\n","1845 [D loss: 0.607161, acc.: 65.62%] [G loss: 1.025463]\n","1846 [D loss: 0.629570, acc.: 64.06%] [G loss: 1.093072]\n","1847 [D loss: 0.554050, acc.: 81.25%] [G loss: 0.948873]\n","1848 [D loss: 0.581972, acc.: 68.75%] [G loss: 1.038480]\n","1849 [D loss: 0.553679, acc.: 79.69%] [G loss: 0.919983]\n","1850 [D loss: 0.566569, acc.: 70.31%] [G loss: 0.964853]\n","1851 [D loss: 0.594400, acc.: 73.44%] [G loss: 1.057771]\n","1852 [D loss: 0.602061, acc.: 65.62%] [G loss: 1.031274]\n","1853 [D loss: 0.533252, acc.: 79.69%] [G loss: 0.988423]\n","1854 [D loss: 0.593914, acc.: 71.88%] [G loss: 0.935263]\n","1855 [D loss: 0.511977, acc.: 79.69%] [G loss: 0.979701]\n","1856 [D loss: 0.596296, acc.: 73.44%] [G loss: 0.988293]\n","1857 [D loss: 0.585723, acc.: 67.19%] [G loss: 0.987370]\n","1858 [D loss: 0.510503, acc.: 75.00%] [G loss: 0.989080]\n","1859 [D loss: 0.593993, acc.: 68.75%] [G loss: 1.148632]\n","1860 [D loss: 0.582959, acc.: 79.69%] [G loss: 1.011865]\n","1861 [D loss: 0.594484, acc.: 65.62%] [G loss: 1.088060]\n","1862 [D loss: 0.618047, acc.: 64.06%] [G loss: 0.942812]\n","1863 [D loss: 0.648048, acc.: 59.38%] [G loss: 0.909977]\n","1864 [D loss: 0.613913, acc.: 68.75%] [G loss: 0.950262]\n","1865 [D loss: 0.571900, acc.: 71.88%] [G loss: 0.927418]\n","1866 [D loss: 0.649792, acc.: 62.50%] [G loss: 0.978172]\n","1867 [D loss: 0.513161, acc.: 79.69%] [G loss: 1.007364]\n","1868 [D loss: 0.597704, acc.: 64.06%] [G loss: 0.963050]\n","1869 [D loss: 0.541548, acc.: 85.94%] [G loss: 1.014105]\n","1870 [D loss: 0.587440, acc.: 67.19%] [G loss: 0.928659]\n","1871 [D loss: 0.547788, acc.: 75.00%] [G loss: 1.003781]\n","1872 [D loss: 0.579537, acc.: 71.88%] [G loss: 0.927447]\n","1873 [D loss: 0.607324, acc.: 68.75%] [G loss: 0.858478]\n","1874 [D loss: 0.634606, acc.: 65.62%] [G loss: 0.952730]\n","1875 [D loss: 0.545048, acc.: 70.31%] [G loss: 0.993591]\n","1876 [D loss: 0.606999, acc.: 62.50%] [G loss: 0.962888]\n","1877 [D loss: 0.622676, acc.: 60.94%] [G loss: 1.035355]\n","1878 [D loss: 0.598476, acc.: 67.19%] [G loss: 0.975170]\n","1879 [D loss: 0.586504, acc.: 70.31%] [G loss: 1.011611]\n","1880 [D loss: 0.598278, acc.: 75.00%] [G loss: 0.983191]\n","1881 [D loss: 0.533526, acc.: 78.12%] [G loss: 1.167781]\n","1882 [D loss: 0.612362, acc.: 65.62%] [G loss: 1.063314]\n","1883 [D loss: 0.586098, acc.: 64.06%] [G loss: 1.083955]\n","1884 [D loss: 0.528986, acc.: 85.94%] [G loss: 1.006776]\n","1885 [D loss: 0.535517, acc.: 73.44%] [G loss: 1.070774]\n","1886 [D loss: 0.581867, acc.: 71.88%] [G loss: 0.980684]\n","1887 [D loss: 0.555043, acc.: 79.69%] [G loss: 1.019281]\n","1888 [D loss: 0.586159, acc.: 70.31%] [G loss: 0.960287]\n","1889 [D loss: 0.571271, acc.: 70.31%] [G loss: 0.918875]\n","1890 [D loss: 0.553135, acc.: 76.56%] [G loss: 0.985355]\n","1891 [D loss: 0.555155, acc.: 73.44%] [G loss: 1.005236]\n","1892 [D loss: 0.534653, acc.: 79.69%] [G loss: 0.961475]\n","1893 [D loss: 0.573859, acc.: 70.31%] [G loss: 0.957605]\n","1894 [D loss: 0.480271, acc.: 84.38%] [G loss: 1.022617]\n","1895 [D loss: 0.550637, acc.: 70.31%] [G loss: 1.030246]\n","1896 [D loss: 0.646411, acc.: 60.94%] [G loss: 1.007012]\n","1897 [D loss: 0.576134, acc.: 70.31%] [G loss: 0.994074]\n","1898 [D loss: 0.611393, acc.: 70.31%] [G loss: 0.989448]\n","1899 [D loss: 0.590746, acc.: 71.88%] [G loss: 0.961164]\n","1900 [D loss: 0.486218, acc.: 82.81%] [G loss: 0.976213]\n","1901 [D loss: 0.613374, acc.: 57.81%] [G loss: 0.981049]\n","1902 [D loss: 0.550625, acc.: 75.00%] [G loss: 0.896689]\n","1903 [D loss: 0.566326, acc.: 71.88%] [G loss: 0.927095]\n","1904 [D loss: 0.518812, acc.: 81.25%] [G loss: 1.073722]\n","1905 [D loss: 0.614461, acc.: 70.31%] [G loss: 0.988773]\n","1906 [D loss: 0.600687, acc.: 73.44%] [G loss: 1.016581]\n","1907 [D loss: 0.501039, acc.: 76.56%] [G loss: 1.033751]\n","1908 [D loss: 0.572690, acc.: 68.75%] [G loss: 0.967949]\n","1909 [D loss: 0.522933, acc.: 78.12%] [G loss: 1.060729]\n","1910 [D loss: 0.592876, acc.: 70.31%] [G loss: 0.937438]\n","1911 [D loss: 0.577424, acc.: 67.19%] [G loss: 0.951808]\n","1912 [D loss: 0.560366, acc.: 75.00%] [G loss: 0.949152]\n","1913 [D loss: 0.552006, acc.: 81.25%] [G loss: 0.956297]\n","1914 [D loss: 0.557606, acc.: 70.31%] [G loss: 0.960506]\n","1915 [D loss: 0.603174, acc.: 64.06%] [G loss: 1.004927]\n","1916 [D loss: 0.536813, acc.: 78.12%] [G loss: 0.956407]\n","1917 [D loss: 0.628988, acc.: 64.06%] [G loss: 1.091489]\n","1918 [D loss: 0.551637, acc.: 75.00%] [G loss: 1.045699]\n","1919 [D loss: 0.518625, acc.: 79.69%] [G loss: 0.996908]\n","1920 [D loss: 0.557083, acc.: 79.69%] [G loss: 1.076787]\n","1921 [D loss: 0.570235, acc.: 75.00%] [G loss: 0.993674]\n","1922 [D loss: 0.553212, acc.: 71.88%] [G loss: 1.059626]\n","1923 [D loss: 0.576783, acc.: 68.75%] [G loss: 1.012744]\n","1924 [D loss: 0.554310, acc.: 75.00%] [G loss: 0.989770]\n","1925 [D loss: 0.553803, acc.: 73.44%] [G loss: 0.973286]\n","1926 [D loss: 0.584598, acc.: 67.19%] [G loss: 1.099045]\n","1927 [D loss: 0.519689, acc.: 78.12%] [G loss: 0.995163]\n","1928 [D loss: 0.636601, acc.: 67.19%] [G loss: 1.063029]\n","1929 [D loss: 0.511229, acc.: 68.75%] [G loss: 1.067707]\n","1930 [D loss: 0.510893, acc.: 81.25%] [G loss: 1.072132]\n","1931 [D loss: 0.558798, acc.: 71.88%] [G loss: 0.998700]\n","1932 [D loss: 0.591061, acc.: 67.19%] [G loss: 0.962360]\n","1933 [D loss: 0.677480, acc.: 54.69%] [G loss: 0.915233]\n","1934 [D loss: 0.526374, acc.: 73.44%] [G loss: 0.918676]\n","1935 [D loss: 0.583637, acc.: 73.44%] [G loss: 1.004540]\n","1936 [D loss: 0.514856, acc.: 84.38%] [G loss: 1.027132]\n","1937 [D loss: 0.580622, acc.: 62.50%] [G loss: 0.911728]\n","1938 [D loss: 0.537927, acc.: 81.25%] [G loss: 0.943271]\n","1939 [D loss: 0.566402, acc.: 82.81%] [G loss: 1.026730]\n","1940 [D loss: 0.585886, acc.: 73.44%] [G loss: 0.928716]\n","1941 [D loss: 0.577316, acc.: 71.88%] [G loss: 0.920593]\n","1942 [D loss: 0.590095, acc.: 68.75%] [G loss: 1.027645]\n","1943 [D loss: 0.618517, acc.: 57.81%] [G loss: 0.953130]\n","1944 [D loss: 0.591181, acc.: 64.06%] [G loss: 1.059223]\n","1945 [D loss: 0.574564, acc.: 73.44%] [G loss: 1.018999]\n","1946 [D loss: 0.523533, acc.: 79.69%] [G loss: 0.957103]\n","1947 [D loss: 0.583655, acc.: 75.00%] [G loss: 1.021622]\n","1948 [D loss: 0.509814, acc.: 76.56%] [G loss: 1.025988]\n","1949 [D loss: 0.631576, acc.: 67.19%] [G loss: 1.012317]\n","1950 [D loss: 0.546776, acc.: 76.56%] [G loss: 0.926540]\n","1951 [D loss: 0.565029, acc.: 75.00%] [G loss: 0.995184]\n","1952 [D loss: 0.595895, acc.: 67.19%] [G loss: 0.975740]\n","1953 [D loss: 0.662705, acc.: 62.50%] [G loss: 0.959848]\n","1954 [D loss: 0.570566, acc.: 75.00%] [G loss: 0.929051]\n","1955 [D loss: 0.587954, acc.: 71.88%] [G loss: 0.993095]\n","1956 [D loss: 0.599273, acc.: 67.19%] [G loss: 1.000422]\n","1957 [D loss: 0.573199, acc.: 73.44%] [G loss: 1.020987]\n","1958 [D loss: 0.565618, acc.: 78.12%] [G loss: 0.936962]\n","1959 [D loss: 0.613986, acc.: 68.75%] [G loss: 0.974618]\n","1960 [D loss: 0.572312, acc.: 71.88%] [G loss: 1.075420]\n","1961 [D loss: 0.586996, acc.: 71.88%] [G loss: 0.911757]\n","1962 [D loss: 0.604155, acc.: 67.19%] [G loss: 0.988577]\n","1963 [D loss: 0.565250, acc.: 73.44%] [G loss: 0.960148]\n","1964 [D loss: 0.617932, acc.: 75.00%] [G loss: 0.967900]\n","1965 [D loss: 0.613676, acc.: 64.06%] [G loss: 1.005241]\n","1966 [D loss: 0.584750, acc.: 70.31%] [G loss: 0.928907]\n","1967 [D loss: 0.596743, acc.: 75.00%] [G loss: 1.050659]\n","1968 [D loss: 0.545932, acc.: 78.12%] [G loss: 1.108442]\n","1969 [D loss: 0.527201, acc.: 79.69%] [G loss: 1.044244]\n","1970 [D loss: 0.565279, acc.: 70.31%] [G loss: 1.020126]\n","1971 [D loss: 0.578036, acc.: 62.50%] [G loss: 1.059403]\n","1972 [D loss: 0.630302, acc.: 62.50%] [G loss: 0.966030]\n","1973 [D loss: 0.600755, acc.: 64.06%] [G loss: 0.969345]\n","1974 [D loss: 0.597812, acc.: 70.31%] [G loss: 1.024154]\n","1975 [D loss: 0.563484, acc.: 78.12%] [G loss: 0.956758]\n","1976 [D loss: 0.628807, acc.: 56.25%] [G loss: 0.971020]\n","1977 [D loss: 0.549042, acc.: 78.12%] [G loss: 1.009092]\n","1978 [D loss: 0.567386, acc.: 68.75%] [G loss: 0.971282]\n","1979 [D loss: 0.564683, acc.: 73.44%] [G loss: 1.021033]\n","1980 [D loss: 0.620511, acc.: 59.38%] [G loss: 1.061082]\n","1981 [D loss: 0.496321, acc.: 79.69%] [G loss: 1.005428]\n","1982 [D loss: 0.534071, acc.: 73.44%] [G loss: 0.922233]\n","1983 [D loss: 0.598596, acc.: 65.62%] [G loss: 0.997924]\n","1984 [D loss: 0.550124, acc.: 68.75%] [G loss: 1.020192]\n","1985 [D loss: 0.540427, acc.: 68.75%] [G loss: 0.959094]\n","1986 [D loss: 0.545967, acc.: 78.12%] [G loss: 0.972553]\n","1987 [D loss: 0.572152, acc.: 68.75%] [G loss: 0.843949]\n","1988 [D loss: 0.551227, acc.: 76.56%] [G loss: 0.936730]\n","1989 [D loss: 0.595727, acc.: 68.75%] [G loss: 0.946613]\n","1990 [D loss: 0.598982, acc.: 65.62%] [G loss: 1.039542]\n","1991 [D loss: 0.619977, acc.: 59.38%] [G loss: 1.060223]\n","1992 [D loss: 0.602678, acc.: 68.75%] [G loss: 1.005116]\n","1993 [D loss: 0.599033, acc.: 65.62%] [G loss: 0.911703]\n","1994 [D loss: 0.605211, acc.: 67.19%] [G loss: 1.083567]\n","1995 [D loss: 0.579366, acc.: 76.56%] [G loss: 1.042237]\n","1996 [D loss: 0.592858, acc.: 73.44%] [G loss: 0.949455]\n","1997 [D loss: 0.529217, acc.: 78.12%] [G loss: 0.988669]\n","1998 [D loss: 0.556349, acc.: 75.00%] [G loss: 0.997834]\n","1999 [D loss: 0.594477, acc.: 75.00%] [G loss: 0.918201]\n","2000 [D loss: 0.597237, acc.: 68.75%] [G loss: 0.900684]\n","2001 [D loss: 0.541283, acc.: 76.56%] [G loss: 0.927091]\n","2002 [D loss: 0.547100, acc.: 73.44%] [G loss: 1.024428]\n","2003 [D loss: 0.533685, acc.: 75.00%] [G loss: 1.004047]\n","2004 [D loss: 0.603919, acc.: 62.50%] [G loss: 0.971657]\n","2005 [D loss: 0.555604, acc.: 76.56%] [G loss: 1.099413]\n"],"name":"stdout"}]}]}